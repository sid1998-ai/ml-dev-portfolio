{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b041bb-6719-453f-a81c-bcbf623a559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775be8b-5329-4420-aec9-2d1dc9f8c9c0",
   "metadata": {},
   "source": [
    "🎯 When to Use CV?\n",
    "✅ Anytime you're:\n",
    "\n",
    "Testing many models\n",
    "\n",
    "Tuning hyperparameters\n",
    "\n",
    "Publishing a real ML project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce62829-9c49-497b-8c1f-ffa9b282ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"titanic_cleaned.csv\")\n",
    "df = df.drop('Embarked_C',axis = 1)\n",
    "#Define features and target\n",
    "X= df.drop('Survived',axis = 1)\n",
    "y = df['Survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee56334-1593-4ecd-9e51-997fb3cbfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why?\n",
    "\n",
    "#Train Test Split gives result on 1 random sample but,\n",
    "#Cross Validation (CV) tests model on multiple folds to make sure it generalizes well\n",
    "\n",
    "#whats multiple folds????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c6075-c093-4407-bca0-cb21a7e004f8",
   "metadata": {},
   "source": [
    "| Benefit                      | Why it matters                                           |\n",
    "| ---------------------------- | -------------------------------------------------------- |\n",
    "| **More Reliable Scores**     | Avoids misleading results from a single train-test split |\n",
    "| **Detects Overfitting**      | Shows if model performs well only on training folds      |\n",
    "| **Works on Small Datasets**  | Makes the most of limited data                           |\n",
    "| **Improves Model Selection** | Helps pick the best model based on average performance   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295813ac-f311-4294-bca1-7156a73261d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.78089888 0.78651685 0.78651685 0.76966292 0.81355932]\n",
      "Average CV Score 0.7874309655303752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#we will get accuracy on cross-validating Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter= 1000)\n",
    "\n",
    "scores = cross_val_score(model , X , y, cv= 5) #cv = 5 folds cross validation\n",
    "\n",
    "print('CV Scores:',scores)\n",
    "print('Average CV Score', scores.mean())\n",
    "\n",
    "#Why cv=5?\n",
    "#Splits data into 5 parts → trains on 4, tests on 1 → repeats 5 times\n",
    "\n",
    "#cv=5 is a safe, standard default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70ce8207-465d-44a1-a612-b46e0971f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.56179775 0.76966292 0.8258427  0.76404494 0.83050847]\n",
      "Average CV Score: 0.7503713578366026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "scores = cross_val_score(model,X,y,cv=5)\n",
    "\n",
    "print(\"CV Scores:\",scores)\n",
    "print(\"Average CV Score:\",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0869f2a-1271-4b33-8568-5dd163b4521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.70224719 0.80337079 0.84831461 0.80898876 0.85875706]\n",
      "Average CV Score: 0.8043356820923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(model,X,y,cv=5)\n",
    "\n",
    "print(\"CV Scores:\",scores)\n",
    "print(\"Average CV Score:\",scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf38ac-2039-433f-b7ae-e403853d1029",
   "metadata": {},
   "source": [
    "🆚 Alternatives to cross_val_score?\n",
    "\n",
    "| Option            | When to Use                                                          |\n",
    "| ----------------- | -------------------------------------------------------------------- |\n",
    "| `cross_val_score` | Quick accuracy check across folds                                    |\n",
    "| `cross_validate`  | If you want multiple metrics (e.g., precision, recall)               |\n",
    "| `StratifiedKFold` | If data is imbalanced and you want each fold to preserve class ratio |\n",
    "| `ShuffleSplit`    | If data is not randomly distributed                                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99128a5a-9e97-4de3-a846-2a8fe3c27f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: {'fit_time': array([0.6504848 , 0.4265151 , 0.30633259, 0.56457138, 0.48725533]), 'score_time': array([0.00806141, 0.00615668, 0.0086143 , 0.00483155, 0.00511217]), 'test_score': array([0.78089888, 0.78651685, 0.78651685, 0.76966292, 0.81355932]), 'train_score': array([0.80590717, 0.80309423, 0.80028129, 0.8045007 , 0.79494382])}\n",
      "[0.78089888 0.78651685 0.78651685 0.76966292 0.81355932]\n",
      "[0.80590717 0.80309423 0.80028129 0.8045007  0.79494382]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(model , X , y, cv= 5, return_train_score = True) #cv = 5 folds cross validation\n",
    "\n",
    "print('CV Scores:',scores)\n",
    "#print('Average CV Score', scores.mean())   does not have mean() attribute\n",
    "print(scores['test_score'])   # Test scores\n",
    "print(scores['train_score'])  # Training scores\n",
    "\n",
    "#Why use cross_validate?\n",
    "#Gives both train and test scores\n",
    "#Can return fit times, scoring multiple metrics, etc.\n",
    "\n",
    "#how does it returns pricison and recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f07957-5785-4009-90b2-5ebf2c029b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.52589631, 0.34317136, 0.26671958, 0.34599519, 0.27939725]), 'score_time': array([0.00463152, 0.00474763, 0.00550771, 0.00444198, 0.00734234]), 'test_score': array([0.78089888, 0.78651685, 0.78651685, 0.76966292, 0.81355932]), 'train_score': array([0.80590717, 0.80309423, 0.80028129, 0.8045007 , 0.79494382])}\n",
      "[0.78089888 0.78651685 0.78651685 0.76966292 0.81355932]\n",
      "[0.80590717 0.80309423 0.80028129 0.8045007  0.79494382]\n"
     ]
    }
   ],
   "source": [
    "#startified K fold (for classification)\n",
    "#StratifiedKFold ensures that each fold has the same proportion of classes as the whole dataset (useful when classes are imbalanced).\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,cross_validate\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "scores = cross_validate(model, X, y, cv = skf, return_train_score = True)\n",
    "\n",
    "print(scores)\n",
    "print(scores['test_score'])   # Test scores\n",
    "print(scores['train_score']) \n",
    "\n",
    "\n",
    "#When to use ??\n",
    "\n",
    "#our target (y) has unequal class counts\n",
    "#Eg: 90% class A, 10% class B → random split may miss class B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2d2f2d6-45d7-4fc3-8608-fa8abdca3d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([1.25413299, 0.29655457, 0.32298446, 0.12730789, 0.39651418]), 'score_time': array([0.00474238, 0.00487208, 0.00444007, 0.00621581, 0.00520086]), 'test_score': array([0.78089888, 0.80898876, 0.80898876, 0.81460674, 0.75706215]), 'train_score': array([0.79746835, 0.80309423, 0.80590717, 0.79606188, 0.80337079])}\n",
      "[0.78089888 0.80898876 0.80898876 0.81460674 0.75706215]\n",
      "[0.79746835 0.80309423 0.80590717 0.79606188 0.80337079]\n"
     ]
    }
   ],
   "source": [
    " #KFold (basic cross-validation)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits= 5 , shuffle = True, random_state = 42)\n",
    "scores = cross_validate(model, X, y, cv = kf, return_train_score = True)\n",
    "print(scores)\n",
    "print(scores['test_score'])   # Test scores\n",
    "print(scores['train_score']) \n",
    "\n",
    "#Question: significance of shufgfle in basic Kfold??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21660939-0102-43c2-a91e-83670977ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.5429287 , 0.15244985, 0.40941525, 0.20128846, 0.27089572]), 'score_time': array([0.00463319, 0.00460196, 0.00492239, 0.00449944, 0.00447416]), 'test_score': array([0.78089888, 0.80337079, 0.7752809 , 0.80898876, 0.71910112]), 'train_score': array([0.79746835, 0.80168776, 0.79887482, 0.80168776, 0.81997187])}\n",
      "[0.78089888 0.80337079 0.7752809  0.80898876 0.71910112]\n",
      "[0.79746835 0.80168776 0.79887482 0.80168776 0.81997187]\n"
     ]
    }
   ],
   "source": [
    "#ShuffleSplit (random train-test splits)\n",
    "#Randomly splits the data n times\n",
    "#Each time: train on 80%, test on 20%\n",
    "#Good for very large datasets\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "ss = ShuffleSplit(n_splits = 5, test_size = 0.2 ,random_state = 42)\n",
    "scores = cross_validate(model, X, y, cv = ss, return_train_score = True)\n",
    "print(scores)\n",
    "print(scores['test_score'])   # Test scores\n",
    "print(scores['train_score'])\n",
    "\n",
    "#Difference between K-fold and ShuffleSplit???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7c36c-88f6-48fb-9dff-80255c40b34e",
   "metadata": {},
   "source": [
    "| Method            | Best For                         | Keeps class balance? | Custom train/test sizes? |\n",
    "| ----------------- | -------------------------------- | -------------------- | ------------------------ |\n",
    "| `cross_val_score` | Quick evaluation                 | ❌ (unless combined)  | ❌                        |\n",
    "| `cross_validate`  | Detailed results (train/test)    | ❌                    | ❌                        |\n",
    "| `StratifiedKFold` | Classification (imbalanced data) | ✅                    | ❌                        |\n",
    "| `KFold`           | General purpose CV               | ❌                    | ❌                        |\n",
    "| `ShuffleSplit`    | Custom train/test splits         | ❌                    | ✅                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f905f64-ed73-4667-ac8a-576186222d7f",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74afa9-6ba1-44ea-ab20-e53862bac096",
   "metadata": {},
   "source": [
    " Why?\n",
    "Models like RandomForest have settings (hyperparameters) like max_depth, n_estimators, etc. Tuning = finding the best combo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "776fb822-accb-474c-96f7-bf4ca19d398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_depth': 6, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best CV Accuracy: 0.8155716371484797\n"
     ]
    }
   ],
   "source": [
    "#lets achieve this HyperParameter Tuning using GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators':[100,200],\n",
    "    'max_depth':[4, 6, 8],\n",
    "    'min_samples_split':[2,5],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv = 5 , scoring = 'accuracy')\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "\n",
    "#while defining these values in param_grid are these random trial and error entries???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9945f6-47ff-4257-beb5-7bcbbcc7bc4f",
   "metadata": {},
   "source": [
    "| Tool                      | When to Use                              |\n",
    "| ------------------------- | ---------------------------------------- |\n",
    "| `GridSearchCV`            | ✅ Try all combos (exhaustive search)     |\n",
    "| `RandomizedSearchCV`      | ✅ Faster for large search spaces         |\n",
    "| `Optuna`, `BayesSearchCV` | Advanced, smarter searching (future use) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76a81b7f-5a09-4243-918e-8eab3e6dac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomizedSearchCV (Faster tuning than GridSearch)\n",
    "#What it does:\n",
    "#Instead of trying every combination (like GridSearch), it randomly samples a few combinations from the parameter grid.\n",
    "#Useful when your grid is large or expensive to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27369272-0e7e-4ebe-97a4-1d965ea5f048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_depth': 6, 'min_samples_split': 2, 'n_estimators': 260}\n",
      "Best CV Accuracy: 0.8189551196597472\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': [4, 6, 8, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions = param_dist,\n",
    "    n_iter = 10,     # Try 10 random combinations\n",
    "    cv = 5,\n",
    "    scoring = 'accuracy',\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "print(\"Best CV Accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c6daa23-fe24-4235-ad3a-129404f6f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from optuna) (1.15.2)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\mahen\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   -------------------- ------------------- 1/2 [optuna]\n",
      "   ---------------------------------------- 2/2 [optuna]\n",
      "\n",
      "Successfully installed colorlog-6.9.0 optuna-4.4.0\n"
     ]
    }
   ],
   "source": [
    "#Optuna\n",
    "!pip install optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee83ebaf-cc79-4470-bd0e-313a36804481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595756cd-39ba-4685-9524-16a014117e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna: The library doing the smart hyperparameter search.\n",
    "#cross_val_score: To evaluate model accuracy using cross-validation.\n",
    "#RandomForestClassifier: The model we’re tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb0e9eac-5954-421a-8af3-fb8e220e4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-29 12:44:15,021] A new study created in memory with name: no-name-74fea0f1-90ed-42c8-833b-860435d09d4e\n",
      "[I 2025-06-29 12:44:27,364] Trial 0 finished with value: 0.8031993905922683 and parameters: {'n_estimators': 409, 'max_depth': 19, 'min_samples_split': 7}. Best is trial 0 with value: 0.8031993905922683.\n",
      "[I 2025-06-29 12:44:42,178] Trial 1 finished with value: 0.8054338856090902 and parameters: {'n_estimators': 467, 'max_depth': 15, 'min_samples_split': 3}. Best is trial 1 with value: 0.8054338856090902.\n",
      "[I 2025-06-29 12:44:44,140] Trial 2 finished with value: 0.8088110201231512 and parameters: {'n_estimators': 102, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 2 with value: 0.8088110201231512.\n",
      "[I 2025-06-29 12:44:49,948] Trial 3 finished with value: 0.8099473116231829 and parameters: {'n_estimators': 205, 'max_depth': 14, 'min_samples_split': 6}. Best is trial 3 with value: 0.8099473116231829.\n",
      "[I 2025-06-29 12:45:01,288] Trial 4 finished with value: 0.8054465816035041 and parameters: {'n_estimators': 437, 'max_depth': 11, 'min_samples_split': 5}. Best is trial 3 with value: 0.8099473116231829.\n",
      "[I 2025-06-29 12:45:15,713] Trial 5 finished with value: 0.806576525106329 and parameters: {'n_estimators': 463, 'max_depth': 10, 'min_samples_split': 7}. Best is trial 3 with value: 0.8099473116231829.\n",
      "[I 2025-06-29 12:45:19,208] Trial 6 finished with value: 0.8088237161175649 and parameters: {'n_estimators': 144, 'max_depth': 14, 'min_samples_split': 8}. Best is trial 3 with value: 0.8099473116231829.\n",
      "[I 2025-06-29 12:45:27,667] Trial 7 finished with value: 0.8099409636259761 and parameters: {'n_estimators': 335, 'max_depth': 17, 'min_samples_split': 10}. Best is trial 3 with value: 0.8099473116231829.\n",
      "[I 2025-06-29 12:45:39,022] Trial 8 finished with value: 0.8155716371484797 and parameters: {'n_estimators': 450, 'max_depth': 13, 'min_samples_split': 10}. Best is trial 8 with value: 0.8155716371484797.\n",
      "[I 2025-06-29 12:45:48,242] Trial 9 finished with value: 0.8088110201231512 and parameters: {'n_estimators': 291, 'max_depth': 12, 'min_samples_split': 3}. Best is trial 8 with value: 0.8155716371484797.\n",
      "[I 2025-06-29 12:45:57,601] Trial 10 finished with value: 0.8054465816035041 and parameters: {'n_estimators': 376, 'max_depth': 4, 'min_samples_split': 9}. Best is trial 8 with value: 0.8155716371484797.\n",
      "[I 2025-06-29 12:46:02,084] Trial 11 finished with value: 0.8178188281597156 and parameters: {'n_estimators': 225, 'max_depth': 6, 'min_samples_split': 5}. Best is trial 11 with value: 0.8178188281597156.\n",
      "[I 2025-06-29 12:46:08,699] Trial 12 finished with value: 0.8178378721513362 and parameters: {'n_estimators': 250, 'max_depth': 6, 'min_samples_split': 5}. Best is trial 12 with value: 0.8178378721513362.\n",
      "[I 2025-06-29 12:46:15,877] Trial 13 finished with value: 0.8166888846568907 and parameters: {'n_estimators': 251, 'max_depth': 5, 'min_samples_split': 5}. Best is trial 12 with value: 0.8178378721513362.\n",
      "[I 2025-06-29 12:46:20,003] Trial 14 finished with value: 0.8166952326540976 and parameters: {'n_estimators': 194, 'max_depth': 7, 'min_samples_split': 4}. Best is trial 12 with value: 0.8178378721513362.\n",
      "[I 2025-06-29 12:46:28,327] Trial 15 finished with value: 0.8088237161175649 and parameters: {'n_estimators': 266, 'max_depth': 7, 'min_samples_split': 2}. Best is trial 12 with value: 0.8178378721513362.\n",
      "[I 2025-06-29 12:46:34,285] Trial 16 finished with value: 0.8099473116231829 and parameters: {'n_estimators': 210, 'max_depth': 7, 'min_samples_split': 5}. Best is trial 12 with value: 0.8178378721513362.\n",
      "[I 2025-06-29 12:46:42,712] Trial 17 finished with value: 0.8155716371484797 and parameters: {'n_estimators': 331, 'max_depth': 5, 'min_samples_split': 6}. Best is trial 12 with value: 0.8178378721513362.\n",
      "[I 2025-06-29 12:46:47,566] Trial 18 finished with value: 0.7998476480670348 and parameters: {'n_estimators': 150, 'max_depth': 9, 'min_samples_split': 4}. Best is trial 12 with value: 0.8178378721513362.\n",
      "[I 2025-06-29 12:46:52,059] Trial 19 finished with value: 0.8099409636259761 and parameters: {'n_estimators': 243, 'max_depth': 6, 'min_samples_split': 7}. Best is trial 12 with value: 0.8178378721513362.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "FrozenTrial(number=12, state=1, values=[0.8178378721513362], datetime_start=datetime.datetime(2025, 6, 29, 12, 46, 2, 86312), datetime_complete=datetime.datetime(2025, 6, 29, 12, 46, 8, 699535), params={'n_estimators': 250, 'max_depth': 6, 'min_samples_split': 5}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=500, log=False, low=100, step=1), 'max_depth': IntDistribution(high=20, log=False, low=4, step=1), 'min_samples_split': IntDistribution(high=10, log=False, low=2, step=1)}, trial_id=12, value=None)\n"
     ]
    }
   ],
   "source": [
    "#Why:\n",
    "#Optuna works by calling this function many times (once per trial).\n",
    "#Each time, it passes a trial object that you use to sample parameters.\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    #Why:\n",
    "    #We’re asking Optuna to try different values:\n",
    "    #n_estimators: number of trees (between 100 and 500)\n",
    "    #max_depth: depth of each tree (between 4 and 20)\n",
    "    #min_samples_split: minimum samples to split a node (between 2 and 10)\n",
    "    #trial.suggest_int(...) tells Optuna to choose an integer in this range.\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    #Why:\n",
    "    #Create a Random Forest model using the parameters sampled by Optuna.\n",
    "    #We keep random_state=42 for reproducibility (same results every time).\n",
    "    score = cross_val_score(clf, X, y, cv=5, scoring='accuracy').mean()\n",
    "    #Why:\n",
    "    #Perform 5-fold cross-validation on the current model.\n",
    "    #cross_val_score() gives us 5 accuracy values — we take the mean as the final score for this trial.\n",
    "    return score\n",
    "    #Why:\n",
    "    #This is the score Optuna will use to decide:\n",
    "    #“Was this parameter combination good or bad?”\n",
    "    #It tries to maximize this return value (in our case, accuracy).\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "#Why:Start a study (an optimization session).\n",
    "#direction='maximize' because we want to maximize accuracy.\n",
    "\n",
    "study.optimize(objective, n_trials=20)\n",
    "#Why:\n",
    "#Run 20 trials (i.e., try 20 different combinations).\n",
    "#It will call the objective() function 20 times with different parameters.\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)\n",
    "#Why:\n",
    "#After all trials, print the best combination of parameters and the score.\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477f670-85ca-4aa9-a74d-4158e8725a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we tried different tuning methods to find best parameters lets use our findings\n",
    "#from GridSearchCV to find which features actually influenced the model's decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0fed6-fbc6-41fc-a1b0-10125424eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance (Random Forest)\n",
    "#When to Use Feature Importance:\n",
    "#To remove useless features\n",
    "#To explain the model\n",
    "#To add to your GitHub README 💪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2f55545-79b7-4fcf-af69-7060288d23a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature  Importance\n",
      "2          Sex    0.431339\n",
      "6         Fare    0.149163\n",
      "1       Pclass    0.137537\n",
      "3          Age    0.108673\n",
      "0  PassengerId    0.075074\n",
      "4        SibSp    0.043218\n",
      "5        Parch    0.027595\n",
      "8   Embarked_S    0.019656\n",
      "7   Embarked_Q    0.007743\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "importances = best_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "feature_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "     'Importance': importances\n",
    "}).sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "print(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91acfd37-30c7-463d-8d9e-09e7aa3d5541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['titanic_final_model.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving OUR BEST MODEL\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(best_model,'titanic_final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f5cf3da-872d-4f18-a1d7-002ad9d119af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to load a model\n",
    "\n",
    "model = joblib.load('titanic_final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c737f99-b123-4c3b-873c-f1832f13c85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
